{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "93e31587",
   "metadata": {},
   "source": [
    "rename this to attention.ipynb\n",
    "\n",
    "In dlvm 22, run:\n",
    "```\n",
    "cd /path/to/your/repo/LUNA/phases/empirical_study\n",
    "conda activate number\n",
    "jupyter noteboook\n",
    "```\n",
    "In local mechine, run:\n",
    "```\n",
    "ssh exp@dlvm22.southcentralus.cloudapp.azure.com -NfL localhost:8888:localhost:8888\n",
    "```\n",
    "In local mechine, open browser with the url dlvm 22 printed, for example http://localhost:8888/?token=0ef5beb7794a909dbf460e1c585d18aa97f1f563df17865f\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "106e2273",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path[0]='<LUNA dir>'#change it to your repo dir\n",
    "import setproctitle\n",
    "\n",
    "setproctitle.setproctitle(\"Expirical\")\n",
    "import argparse\n",
    "import os, json\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from bertviz import head_view, model_view\n",
    "from phases.downstream_tasks.TAT.tagop.roberta_num import RobertaNum\n",
    "from number_tokenizer import prepare_tokenizer\n",
    "from phases.downstream_tasks.TAT.data.roberta_dataset import collate as roberta_collate\n",
    "from phases.downstream_tasks.TAT.data.tapas_dataset import collate as tapas_collate\n",
    "from phases.downstream_tasks.TAT.tagop.tapas_num import TapasNum\n",
    "from phases.downstream_tasks.TAT.options import add_train_args, add_bert_args, DEFAULT_DATA_DIR, TABLE_ENCODERS\n",
    "import pickle as pk\n",
    "import random\n",
    "import numpy as np\n",
    "from sklearn import manifold\n",
    "\n",
    "\n",
    "def func(id, data):\n",
    "    if id == 'r':\n",
    "        return random.sample(data, 1)[0]\n",
    "    for x in data:\n",
    "        if x[\"question_id\"] == id:\n",
    "            return x\n",
    "\n",
    "\n",
    "def load_data(args, tokenizer):\n",
    "    name = ['ori', 'num', 'both']\n",
    "    with open(os.path.join(args.input_dir, f\"{args.encoder}_{name[args.use_numtok]}_cache_dev.pkl\"),\n",
    "              'rb') as f:\n",
    "        dev_data = pk.load(f)\n",
    "    kept_keys = tuple(args.kept_keys.split(',')) if args.kept_keys != '' else ()\n",
    "    if args.encoder == 'roberta':\n",
    "        collate = lambda x: roberta_collate(x, tokenizer, kept_keys)\n",
    "    elif args.encoder == 'tapas':\n",
    "        collate = lambda x: tapas_collate(x, tokenizer, kept_keys)\n",
    "    return dev_data, collate\n",
    "\n",
    "\n",
    "def choose_model(args):\n",
    "    tokenizer = prepare_tokenizer(args.encoder, model_dir=args.model_dir,\n",
    "                                  redirect_huggingface_cache=args.redirect_huggingface_cache)\n",
    "    if args.encoder == 'roberta':\n",
    "        clazz = RobertaNum\n",
    "        hidden_size = 1024\n",
    "    elif args.encoder == 'tapas':\n",
    "        clazz = TapasNum\n",
    "        hidden_size = 768\n",
    "    network = clazz(\n",
    "        tokenizer=tokenizer,\n",
    "        hidden_size=hidden_size,\n",
    "        dropout_prob=args.dropout,\n",
    "        use_newtag=args.use_newtag,\n",
    "        model_name=args.model_name,\n",
    "        checkpoint_path=args.checkpoint_path,\n",
    "        model_dir=args.model_dir,\n",
    "        redirect_huggingface_cache=args.redirect_huggingface_cache\n",
    "    )\n",
    "    return tokenizer, network\n",
    "\n",
    "@torch.no_grad()\n",
    "def get_attention(collate, sample, network, tokenizer, max_length):\n",
    "    batch = collate([sample])\n",
    "\n",
    "    if \"token_type_ids_for_encoder\" not in batch:\n",
    "        batch[\"token_type_ids_for_encoder\"] = torch.zeros_like(batch[\"input_ids\"])\n",
    "    if network.use_numbed:\n",
    "        num_embed = network.numbed.embedding(batch[\"input_ids\"], batch[\"numtok_dict\"], tokenizer.num_token_id)\n",
    "    else:\n",
    "        num_embed = None\n",
    "    txt_embed = network.encoder.embeddings.word_embeddings(batch[\"input_ids\"])\n",
    "    if num_embed is None:\n",
    "        input_embedding = txt_embed\n",
    "    else:\n",
    "        input_embedding = txt_embed + num_embed\n",
    "    output = network.encoder(\n",
    "        inputs_embeds=input_embedding,\n",
    "        attention_mask=batch[\"attention_mask\"],\n",
    "        token_type_ids=batch[\"token_type_ids_for_encoder\"],\n",
    "        output_attentions=True,\n",
    "        output_hidden_states=True,\n",
    "        return_dict=True\n",
    "    )\n",
    "    attention=output.attentions\n",
    "    hidden_states=output.hidden_states\n",
    "    sentence_b_start = batch[\"token_type_ids\"][0].tolist().index(1)\n",
    "    input_id_list = batch[\"input_ids\"][0].tolist()  # Batch index 0\n",
    "    tokens = tokenizer.convert_ids_to_tokens(input_id_list)\n",
    "    \n",
    "    ### attention ###\n",
    "    slice_att=[any([i in token for i in '0123456789']) or token.lower()=='[num]' \\\n",
    "           or j<sentence_b_start for j,token in enumerate(tokens)] \n",
    "    attention_att=[att[:,:,slice_att][:,:,:,slice_att] for att in attention]\n",
    "    tokens_att=[token for flag,token in zip(slice_att,tokens) if flag]\n",
    "    max_length=max(max_length,sentence_b_start+10)\n",
    "    attention_att=[att[:,:,:max_length,:max_length]/att[:,:,:max_length,:max_length].sum(-1,keepdim=True) for att in attention_att]\n",
    "    tokens_att=tokens_att[:max_length]\n",
    "    \n",
    "    ### tsne ###\n",
    "    slice_tsne=[any([i in token for i in '0123456789']) or token.lower()=='[num]' for j,token in enumerate(tokens)] \n",
    "    input_embedding=input_embedding[0,slice_tsne].numpy()\n",
    "    hidden_states_tsne=[state[0,slice_tsne].numpy() for state in hidden_states]\n",
    "    tokens_tsne=[token for flag,token in zip(slice_tsne,tokens) if flag]\n",
    "    \n",
    "    return attention_att, tokens_att, sentence_b_start, \\\n",
    "            [input_embedding]+hidden_states_tsne,tokens_tsne\n",
    "\n",
    "def draw_tsne(all_embeddings_tsne,tokens_tsne,number_strings,tsne_layers):\n",
    "    dim=all_embeddings_tsne[0].shape[1]\n",
    "    tsne = manifold.TSNE(n_components=2, init='pca', random_state=501)\n",
    "    slice_numtok=np.array([token.lower()=='[num]' for j,token in enumerate(tokens_tsne)],dtype=np.bool)\n",
    "    for i in tsne_layers:\n",
    "        X_tsne = tsne.fit_transform(all_embeddings_tsne[i])\n",
    "        x_min, x_max = X_tsne.min(0), X_tsne.max(0)\n",
    "        X_norm = (X_tsne - x_min) / (x_max - x_min) \n",
    "        plt.figure(figsize=(10,10))\n",
    "        k=0\n",
    "        for j,token in enumerate(tokens_tsne):\n",
    "            if token.lower()=='[num]':\n",
    "                plt.text(X_norm[j,0],X_norm[j,1],number_strings[k],c='#ff0000',fontsize=15)\n",
    "                k+=1\n",
    "            else:\n",
    "                plt.text(X_norm[j,0],X_norm[j,1],token,c='#00ff00',fontsize=15)\n",
    "        plt.scatter([0,1],[1,0],c='#ff0000',label='[NUM]')\n",
    "        plt.scatter([0,1],[1,0],c='#00ff00',label='language')\n",
    "        plt.legend(loc=\"upper right\")\n",
    "        plt.xticks([])\n",
    "        plt.yticks([])\n",
    "        plt.title('layer %d'%i)\n",
    "        plt.show()\n",
    "    \n",
    "def load_args():\n",
    "    parser_op = argparse.ArgumentParser(\"TagOp training.\")\n",
    "    add_train_args(parser_op)\n",
    "    add_bert_args(parser_op)\n",
    "    parser_op.add_argument(\"--encoder\", type=str, default='roberta', choices=TABLE_ENCODERS,\n",
    "                           help='which encoder to use, default roberta')\n",
    "    parser_op.add_argument(\"--input_dir\", type=str, default=DEFAULT_DATA_DIR, help='where the data is')\n",
    "    parser_op.add_argument(\"--ckpt\", type=str, default='',\n",
    "                           help='if empty(default), nothing will happen; else, a TATQA ckpt will be used')\n",
    "    parser_op.add_argument(\"--use_numtok\", type=int, default=1,\n",
    "                           help='whether to use numtok, default 1; 0 means origin; 2 means both origin and numtok')\n",
    "    parser_op.add_argument(\"--use_newtag\", type=int, default=0,\n",
    "                           help='whether to use mean pooling for tagging probability, default 0')\n",
    "    parser_op.add_argument(\"--kept_keys\", type=str, default='batch_token_ids,batch_seq_len',\n",
    "                           help=\"param of Numtok.collate\")\n",
    "    parser_op.add_argument(\"--model_name\", type=str, default='',\n",
    "                           help=\"if empty(default),default model will be used;'zero' to use zero numbed\")\n",
    "    parser_op.add_argument(\"--checkpoint_path\", type=str, default='',\n",
    "                           help=\"if empty(defaut), default numbed will be used; if 'random', random numbed will be used\")\n",
    "    args = parser_op.parse_args()\n",
    "\n",
    "    if args.model_name == '':\n",
    "        args.model_name = 'CharLSTM_base' if args.encoder == 'tapas' else 'CharLSTM_large'\n",
    "    if args.model_name != 'zero':\n",
    "        if args.checkpoint_path != '':\n",
    "            if args.checkpoint_path == 'random':\n",
    "                args.checkpoint_path = ''\n",
    "        else:\n",
    "            assert args.model_name in {'CharLSTM_base', 'CharLSTM_large'}\n",
    "            args.checkpoint_path = '/storage/luna-models/numbed-ckpt/%s.pt' % args.model_name\n",
    "\n",
    "    return args"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "954a3b15",
   "metadata": {},
   "source": [
    "change the cmd as you wish to test other models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d817c547",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shlex\n",
    "cmd = f\"\"\"\n",
    "        --encoder=roberta \\\n",
    "        --model_name=CharLSTM_9M_large \\\n",
    "        --checkpoint_path=random \\\n",
    "        --use_numtok=2 \\\n",
    "        --ckpt=/storage/luna-models/checkpoint/empirical/new_sota.pth\n",
    "        \"\"\"\n",
    "sys.argv = shlex.split(cmd)\n",
    "args = load_args()\n",
    "tokenizer, network = choose_model(args)\n",
    "network.eval()\n",
    "dev_data, collate = load_data(args, tokenizer)\n",
    "if args.ckpt != '':\n",
    "    assert os.path.exists(args.ckpt)\n",
    "    network.load_state_dict(torch.load(args.ckpt, map_location='cpu'), strict=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af26b911",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "line='005da8e4-571e-4612-9333-6c77d03ed838'#please input question_id(r to be random)\n",
    "attention_max_length=128\n",
    "tsne_layers=[0,8,16,24]\n",
    "sample = func(line, dev_data)\n",
    "print(sample['answer_dict'])\n",
    "if sample is None:\n",
    "    print('not find %s' % line)\n",
    "else:\n",
    "    print('question_id:%s' % sample['question_id'])\n",
    "    attention_att, tokens_att, sentence_b_start, \\\n",
    "            all_embeddings_tsne,tokens_tsne = get_attention(collate, sample, network, tokenizer,attention_max_length)\n",
    "    head_view(attention_att, tokens_att, sentence_b_start)\n",
    "    draw_tsne(all_embeddings_tsne,tokens_tsne,sample[\"number_strings\"],tsne_layers)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
