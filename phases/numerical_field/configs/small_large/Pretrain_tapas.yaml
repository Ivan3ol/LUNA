encoder: tapas
step_size: 100
epochs: 2
batch_size: 3
gradient_accumulation_steps: 4
mlm_probability: 0.15
momentum: 0.995
alpha: 0.4
max_seq_length: 400
numbed_model: CharLSTM_base
numbed_ckpt: data/ckpt/numbed-ckpt/CharLSTM_base.pt

optimizer: {lr: 5e-5, weight_decay: 0.02}
schedular: {T_0: 210, T_mult: 2, eta_min: 1e-5}
warm_up_steps: 5250
save_by_steps: 7500

dataset_dir: [data/PretrainDataset/small,data/PretrainDataset/large]
