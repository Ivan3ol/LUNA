encoder: roberta
step_size: 23
epochs: 4
batch_size: 1
gradient_accumulation_steps: 3
mlm_probability: 0.15
momentum: 0.995
alpha: 0.4
max_seq_length: 512
numbed_model: CharLSTM_large
numbed_ckpt: data/ckpt/numbed-ckpt/CharLSTM_large.pt
save_by_epochs: 2
optimizer: {lr: 4.2e-5, weight_decay: 0.02}
schedular: {T_0: 58, T_mult: 2, eta_min: 1.4e-5}
warm_up_steps: 333

dataset_dir: [data/PretrainDataset/small]



